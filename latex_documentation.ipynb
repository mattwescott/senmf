{
 "metadata": {
  "name": "",
  "signature": "sha256:dff4399eecf1c3eaef27c54e8be44ac934fe68f410a48334d408e85162b314b2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Definitions\n",
      "\n",
      "#### Raw Data\n",
      "\n",
      "$X$ is a data matrix with dimension $N$x$F$\n",
      "\n",
      "$N$ is the number of timesteps\n",
      "\n",
      "$F$ is the number of features (perhaps spectrogram bins)\n",
      "\n",
      "#### Decomposition\n",
      "\n",
      "We will decompose the data matrix $X$ by using $K$ basis functions. Each basis function is a window of length $W$\n",
      "\n",
      "$A$ is an activation matrix with dimension $K$x$N$\n",
      "\n",
      "$D$ is a dictionary tensor with dimension $K$x$W$x$F$\n",
      "\n",
      "So $D_0$ will be the first basis window with dimension $W$x$F$\n",
      "\n",
      "### Computing the decomposition\n",
      "\n",
      "#### Goal\n",
      "\n",
      "We will simultanously try to find a good activation matrix and a good dictionary matrix. In other words, we try to find a good set of basis windows, and at the same time the coefficients that approximate the original data matrix using those basis windows. Our approximation of X will be...\n",
      "\n",
      "$\\widetilde{X} = \\sum\\limits_{i=0}^{K-1} D_i \\ast A_i$ \n",
      "\n",
      "Where $\\ast$ represents convolution\n",
      "\n",
      "In creating the approximation we'll attempt to capture the parts of the data we care about, and leave out the rest. This is done by choosing an appropriate cost function. Or, looking from a different angle, by choosing a noise model. We'll start with the KL-Divergence, which, at least in the non-convolutive case, corresponds to a poisson noise model. \n",
      "\n",
      "so we'll attempt to minimize  $D(X| \\widetilde{X} ) = \\sum{x_{ij}log(\\frac{x_{ij}}{\\tilde{x_{ij}}})}$\n",
      "\n",
      "This model is convex in either $A$ or $D$, but not in both. To exploit this convexity we alternate between updating $A$ with $D$ held fixed, and updating $D$ with $A$ held fixed\n",
      "\n",
      "#### Start with non-convolutive case\n",
      "\n",
      "As it turns out, using the simple multiplicate updates from [this paper](http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf), the updates to both $A$ and $D$ depend on the current approximation only through the ratio...\n",
      "\n",
      "$R=\\frac{X}{\\widetilde{X}}$\n",
      "\n",
      "This can be seen as a multiplicative residual. If we are trying to update $A$ with $D$ held fixed, we need to decide how to distribute this residual to the parameters of $A$. In other words, how to change the activations to bring $\\widetilde{X}$ closer to $X$\n",
      "\n",
      "To do this we take each activation coefficient independently, and ask whether the corresponding basis covaries with the parts of R that it can influence. So for activation $t$, basis $j$ and offset $t'$, we want to know...\n",
      "\n",
      "$\\widehat{D_{j}} \\cdot R_{t}$\n",
      "\n",
      "In words, we want to know how much of the variation in $R$ can be explained by a particular basis window\n",
      "\n",
      "Similarly, when updating the dictionaries, we can take each dictionary parameter and ask whether its corresponding activations covary with $R$\n",
      "\n",
      "$\\widehat{A_j} \\cdot R_t$\n",
      "\n",
      "We can use this info to update both $A$ and $D$\n",
      "\n",
      "$ A_{j} *= \\widehat{D_{j}} \\cdot R_{t}$\n",
      "\n",
      "$D_{j} *= \\widehat{A_j} \\cdot R_t$\n",
      "\n",
      "#### Moving to convolutive case\n",
      "\n",
      "When the basis windows are smaller than $X$, and we want activations for different offsets, this gets a bit more complex. It seems to work well to treat each offset independently in the updates. \n",
      "\n",
      "$ A_{jt} *= \\widehat{D_{jt'}} \\cdot R_{t+t'}$\n",
      "\n",
      "$D_{j} *= \\widehat{A_j,t+t'} \\cdot R_{t$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}